{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part B.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jUWOu9rd4GRQ","colab_type":"code","outputId":"f8eb1ff4-620a-4492-cd2c-43a9dcd530f1","executionInfo":{"status":"ok","timestamp":1573525677542,"user_tz":-480,"elapsed":23992,"user":{"displayName":"Png Qun Jia","photoUrl":"","userId":"16238478669356248828"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# /content/drive/My Drive/CZ4042 Neural Network project/project 2/Part B/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ab1JK8YQ0yIi","colab_type":"text"},"source":["# Import Libraries\n"]},{"cell_type":"code","metadata":{"id":"lxUg5BaW4ENf","colab_type":"code","outputId":"6de943ae-6451-4ff5-9992-e9df282dc989","executionInfo":{"status":"ok","timestamp":1573525679308,"user_tz":-480,"elapsed":25694,"user":{"displayName":"Png Qun Jia","photoUrl":"","userId":"16238478669356248828"}},"colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["import numpy as np\n","import pandas\n","import tensorflow as tf\n","import csv\n","import matplotlib.pyplot as plt\n","import time\n","import datetime\n","from tqdm import tqdm\n","\n","MAX_DOCUMENT_LENGTH = 100\n","N_FILTERS = 10\n","HIDDEN_SIZE = 20 #RNN\n","POOLING_WINDOW = 4 #CNN\n","POOLING_STRIDE = 2 #CNN\n","MAX_LABEL = 15\n","batch_size = 128\n","EMBEDDING_SIZE = 20\n","lr = 0.01\n","dropout = 0.9\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","seed = 10\n","tf.set_random_seed(seed)\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"K0b7aeMALPsn","colab_type":"text"},"source":["# CNN"]},{"cell_type":"code","metadata":{"id":"0_F8ZwvGHA7G","colab_type":"code","colab":{}},"source":["def cnn_model(x,with_dropout,idType):\n","\n","  if idType == 'char':\n","    input_layer = tf.reshape(\n","        tf.one_hot(x, 256), [-1, MAX_DOCUMENT_LENGTH, 256, 1])\n","    FILTER_SHAPE1 = [20, 256]\n","    FILTER_SHAPE2 = [20, 1]\n","  elif idType == 'word':\n","    word_vectors = tf.contrib.layers.embed_sequence(x, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n","    input_layer = tf.reshape(word_vectors, [-1, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE, 1])\n","    FILTER_SHAPE1 = [20,20]\n","    FILTER_SHAPE2 = [20,1]\n","    \n","  with tf.variable_scope('CNN_Layer1'):\n","    conv1 = tf.layers.conv2d(\n","        input_layer,\n","        filters=N_FILTERS,\n","        kernel_size=FILTER_SHAPE1,\n","        padding='VALID',\n","        activation=tf.nn.relu)\n","    pool1 = tf.layers.max_pooling2d(\n","        conv1,\n","        pool_size=POOLING_WINDOW,\n","        strides=POOLING_STRIDE,\n","        padding='SAME')\n","    if with_dropout:\n","      pool1 = tf.nn.dropout(pool1, dropout)\n","    \n","  with tf.variable_scope('CNN_Layer2'):\n","    conv2 = tf.layers.conv2d(\n","        pool1,\n","        filters=N_FILTERS,\n","        kernel_size=FILTER_SHAPE2,\n","        padding='VALID',\n","        activation=tf.nn.relu)\n","    pool2 = tf.layers.max_pooling2d(\n","        conv2,\n","        pool_size=POOLING_WINDOW,\n","        strides=POOLING_STRIDE,\n","        padding='SAME')\n","    \n","    if with_dropout:\n","        pool2 = tf.nn.dropout(pool2, dropout)\n","\n","    pool2 = tf.squeeze(tf.reduce_max(pool2, 1), squeeze_dims=[1])\n","\n","  logits = tf.layers.dense(pool2, MAX_LABEL, activation=None)\n","\n","  return input_layer, logits"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYHjTJWALSln","colab_type":"text"},"source":["# RNN"]},{"cell_type":"code","metadata":{"id":"joLRAgme_4j1","colab_type":"code","colab":{}},"source":["def create_RNN_cell(cell_type):\n","      if cell_type == 'GRU':\n","          cell = tf.nn.rnn_cell.GRUCell(HIDDEN_SIZE)\n","      elif cell_type == 'RNN':\n","          cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n","      elif cell_type == 'LSTM':\n","          cell = tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)\n","      return cell\n","\n","def rnn_model(x, with_dropout,idType,cell_type,num_layers,gradient_clipping):\n","      if idType == \"char\":\n","          id_vectors = tf.one_hot(x, 256)\n","          id_list = tf.unstack(id_vectors, axis=1)\n","      elif idType == \"word\":\n","          id_vectors = tf.contrib.layers.embed_sequence(x, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n","          id_list = tf.unstack(id_vectors, axis=1)\n","\n","      if num_layers>1:\n","          cell = tf.contrib.rnn.MultiRNNCell([create_RNN_cell(cell_type) for _ in range(num_layers)])\n","      else:\n","          cell = create_RNN_cell(cell_type)\n","\n","      if with_dropout:\n","          cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=dropout, output_keep_prob=dropout)\n","\n","      _, encoding = tf.nn.static_rnn(cell, id_list, dtype=tf.float32)\n","      if isinstance(encoding, tuple):\n","                encoding = encoding[-1]\n","\n","      logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n","\n","      return logits, id_list"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhaKd_f3LYwN","colab_type":"text"},"source":["# Read Characters"]},{"cell_type":"code","metadata":{"id":"UFKZwIJNK4Zc","colab_type":"code","colab":{}},"source":["def read_data_chars():\n","  \n","  x_train, y_train, x_test, y_test = [], [], [], []\n","\n","  with open('/content/drive/My Drive/CZ4042 Neural Network project/project 2/Part B/train_medium.csv', encoding='utf-8') as filex:\n","    reader = csv.reader(filex)\n","    for row in reader:\n","      x_train.append(row[1])\n","      y_train.append(int(row[0]))\n","\n","  with open('/content/drive/My Drive/CZ4042 Neural Network project/project 2/Part B/test_medium.csv', encoding='utf-8') as filex:\n","    reader = csv.reader(filex)\n","    for row in reader:\n","      x_test.append(row[1])\n","      y_test.append(int(row[0]))\n","  \n","  x_train = pandas.Series(x_train)\n","  y_train = pandas.Series(y_train)\n","  x_test = pandas.Series(x_test)\n","  y_test = pandas.Series(y_test)\n","  \n","  \n","  char_processor = tf.contrib.learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n","  x_train = np.array(list(char_processor.fit_transform(x_train)))\n","  x_test = np.array(list(char_processor.transform(x_test)))\n","  y_train = y_train.values\n","  y_test = y_test.values\n","  \n","  return x_train, y_train, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZzP6jsKLcuH","colab_type":"text"},"source":["# Read Words"]},{"cell_type":"code","metadata":{"id":"0nR1Fg28Kc9I","colab_type":"code","colab":{}},"source":["def read_data_words():\n","  \n","  x_train, y_train, x_test, y_test = [], [], [], []\n","  \n","  with open('/content/drive/My Drive/CZ4042 Neural Network project/project 2/Part B/train_medium.csv', encoding='utf-8') as filex:\n","    reader = csv.reader(filex)\n","    for row in reader:\n","      x_train.append(row[2])\n","      y_train.append(int(row[0]))\n","\n","  with open(\"/content/drive/My Drive/CZ4042 Neural Network project/project 2/Part B/test_medium.csv\", encoding='utf-8') as filex:\n","    reader = csv.reader(filex)\n","    for row in reader:\n","      x_test.append(row[2])\n","      y_test.append(int(row[0]))\n","  \n","  x_train = pandas.Series(x_train)\n","  y_train = pandas.Series(y_train)\n","  x_test = pandas.Series(x_test)\n","  y_test = pandas.Series(y_test)\n","  y_train = y_train.values\n","  y_test = y_test.values\n","  \n","  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n","      MAX_DOCUMENT_LENGTH)\n","\n","  x_transform_train = vocab_processor.fit_transform(x_train)\n","  x_transform_test = vocab_processor.transform(x_test)\n","\n","  x_train = np.array(list(x_transform_train))\n","  x_test = np.array(list(x_transform_test))\n","\n","  no_words = len(vocab_processor.vocabulary_)\n","  print('Total words: %d' % no_words)\n","\n","  return x_train, y_train, x_test, y_test, no_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnOJcIvALnT-","colab_type":"text"},"source":["# Plot Graph"]},{"cell_type":"code","metadata":{"id":"8FEs_yKIKdDf","colab_type":"code","colab":{}},"source":["def get_title(idType,networkType, with_dropout,cell_type=None,num_layers=1,gradient_clipping=False):\n","  title = networkType\n","  if networkType == \"RNN\":\n","    title += \" (\" + cell_type + \")\"\n","  title += \" \" + idType + \" Classifier\"\n","  if num_layers>1:\n","    title+= \" (\" + str(num_layers) + \" layers)\"\n","\n","  if gradient_clipping:\n","    title +=  \" with gradient clipping\" \n","\n","\n","  if gradient_clipping and with_dropout:\n","    title+= \" and dropout\"\n","  elif with_dropout:\n","    title+= \" with dropout\"\n","\n","  return title\n","\n","def plotGraph(loss,test_acc,idType,networkType, with_dropout,cell_type,num_layers,gradient_clipping,time_taken):\n","  plt.figure(1)\n","  \n","  title = get_title(idType,networkType, with_dropout,cell_type,num_layers,gradient_clipping)\n","  max_acc = max(test_acc)      \n","  plt.suptitle(title)\n","  plt.title(\"time taken: \" + time_taken + \" best acc: \" + str(max_acc), fontsize=9)\n","  plt.xlabel(str(no_epochs) + ' iterations')\n","  plt.ylabel('Error')\n","  plt.plot(range(no_epochs), loss, c=\"r\")\n","  plt.plot(range(no_epochs), test_acc, c=\"b\")\n","  plt.legend([\"train error\", \"test accuracy\"],loc='upper left')\n","  title=title.replace(\" \", \"_\") + \".png\"\n","  plt.savefig(title)\n","  plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-BL89QgKfxl","colab_type":"text"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"VdwM5etPKdGn","colab_type":"code","colab":{}},"source":["def textClassifier(idType,networkType, with_dropout,cell_type=None,num_layers=1,gradient_clipping=False):\n","  global n_words\n","\n","  if idType == \"char\":\n","    x_train, y_train, x_test, y_test = read_data_chars()\n","  elif idType == \"word\":\n","    x_train, y_train, x_test, y_test, n_words= read_data_words()\n","\n","  print(\"length of train data:\", len(x_train))\n","  print(\"length of test data:\", len(x_test))\n","  print(\"Number of epochs:\", no_epochs)\n","\n","  # Create the model\n","  x = tf.placeholder(tf.int64, [None, MAX_DOCUMENT_LENGTH])\n","  y_ = tf.placeholder(tf.int64)\n","\n","  if networkType == \"CNN\":\n","    inputs, logits = cnn_model(x, with_dropout,idType)\n","  elif networkType == \"RNN\":\n","    logits, word_list = rnn_model(x, with_dropout,idType,cell_type,num_layers,gradient_clipping)\n","\n","  # Optimizer\n","  entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(y_, MAX_LABEL), logits=logits))\n","  \n","  if gradient_clipping:\n","    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","    gvs = optimizer.compute_gradients(entropy)\n","    capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n","    train_op = optimizer.apply_gradients(capped_gvs)\n","\n","  else:\n","    train_op = tf.train.AdamOptimizer(lr).minimize(entropy)\n","\n","  # Accuracy\n","  correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), y_), tf.float32)\n","  accuracy = tf.reduce_mean(correct_prediction)\n","\n","  # training\n","  loss = []\n","  test_acc = []\n","  index = np.arange(len(x_train))\n","\n","  timer = time.time()\n","  sess = tf.Session()\n","  sess.run(tf.global_variables_initializer())\n","\n","  for e in tqdm(range(no_epochs)):\n","    np.random.shuffle(index)\n","    x_train, y_train = x_train[index], y_train[index]\n","\n","    for s in range(0, x_train.shape[0]-batch_size, batch_size):\n","      sess.run(train_op, {x: x_train[s:s+batch_size], y_: y_train[s:s+batch_size]})\n","    \n","    loss_ = sess.run(entropy, {x: x_train, y_: y_train})\n","    test_acc_ = accuracy.eval(session=sess, feed_dict={x:x_test, y_: y_test})\n","    \n","    loss.append(loss_)\n","    test_acc.append(test_acc_)\n","  \n","  sess.close()\n","  time_taken = time.time() - timer\n","  time_taken = str(datetime.timedelta(seconds=time_taken)).split(\".\")[0]\n","  print('Time Taken: ' + time_taken)\n","\n","  plotGraph(loss,test_acc,idType,networkType, with_dropout,cell_type,num_layers,gradient_clipping,time_taken)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_0Uewxb9qLe","colab_type":"code","outputId":"42555084-ebfd-4f32-f8bd-a3079efc8f37","executionInfo":{"status":"ok","timestamp":1573528714537,"user_tz":-480,"elapsed":867422,"user":{"displayName":"Png Qun Jia","photoUrl":"","userId":"16238478669356248828"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import files\n","import time\n","def downloadfile(title):\n","  title=title.replace(\" \", \"_\") + \".png\"\n","  printed = False\n","  try:\n","    files.download(\"/content/\" + title) \n","  except:\n","    pass\n","\n","\n","\n","no_epochs = 500\n","\n","# Question 1 to 5\n","def qns5(c):\n","  idtypeList = ['char', 'word']\n","  networktypeList = ['CNN', 'RNN']\n","  with_dropoutList = [False,True]\n","  i = 0\n","  cell_type = \"GRU\"\n","  for idtype in idtypeList:\n","    for networktype in networktypeList:\n","      for with_dropout in with_dropoutList:\n","        i += 1\n","        if i <= c:\n","          continue\n","        elif i > c+1:\n","          return\n","        tf.reset_default_graph() \n","        title = get_title(idtype,networktype, with_dropout, cell_type = cell_type)\n","        print(title)\n","        textClassifier(idtype,networktype, with_dropout, cell_type = cell_type)\n","        downloadfile(title)\n","\n","# Question 6\n","def qns6a(c):\n","  celltypeList = ['RNN', 'LSTM']\n","  idtypeList = ['char', 'word']\n","  i=0\n","  with_dropout = False\n","  networktype = \"RNN\"\n","  num_layer = 1\n","  gradient_clipping = False\n","  for celltype in celltypeList:\n","    for idtype in idtypeList:\n","      i += 1\n","      if i <= c:\n","        continue\n","      elif i > c+1:\n","        return\n","      title = get_title(idtype,networktype, with_dropout, cell_type = celltype, num_layers=num_layer, gradient_clipping=gradient_clipping)\n","      print(title)\n","      tf.reset_default_graph() \n","      textClassifier(idtype,networktype, with_dropout, cell_type = celltype, num_layers=num_layer, gradient_clipping=gradient_clipping)\n","      downloadfile(title)\n","  print(\"Question 6a done\")\n","\n","def qns6b_6c(c):\n","  num_layerList = [1,2]\n","  gradient_clippingList = [False,True]\n","  idtypeList = ['char', 'word']\n","  num=0\n","  i = 0\n","  with_dropout = False\n","  networktype = \"RNN\"\n","  celltype = 'GRU'\n","  for num_layer in num_layerList:\n","    for gradient_clipping in gradient_clippingList:\n","      for idtype in idtypeList:\n","        title = get_title(idtype,networktype, with_dropout, cell_type = celltype, num_layers=num_layer, gradient_clipping=gradient_clipping)\n","        print(title)\n","        i += 1\n","        if i <= c:\n","          continue\n","        elif i > c+1:\n","          return(\"a\")\n","        # num+= 1\n","        # if num <= 2:\n","        #   print(\"Done in question 1 to 5\") # done in question 3 and 4 already\n","        #   continue\n","        # elif num >= 6:# the rest not required by assigment\n","        #   print(\"Question 6b and 6c done\")\n","        #   return\n","        tf.reset_default_graph() \n","        textClassifier(idtype,networktype, with_dropout, cell_type = celltype, num_layers=num_layer, gradient_clipping=gradient_clipping)\n","        downloadfile(title)\n","\n","# qns5(7)        \n","# qns6a(3)\n","# qns6b_6c(6)\n","\n","\n","\n","######################\n","tf.reset_default_graph() \n","print(get_title('char','RNN', False, cell_type = 'LSTM', num_layers=1, gradient_clipping=False))\n","textClassifier('char','RNN', False, cell_type = 'LSTM', num_layers=1, gradient_clipping=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RNN (LSTM) char Classifier\n","length of train data: 5600\n","length of test data: 700\n","Number of epochs: 500\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 500/500 [14:17<00:00,  1.70s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Time Taken: 0:14:18\n"],"name":"stdout"}]}]}